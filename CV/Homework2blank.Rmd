---
title: "Homework 2 R markdown"
author: "Adam Hendel"
date: '`r Sys.Date()`'
output:
  word_document:
    fig_height: 4
    fig_width: 4.5
  pdf_document:
    fig_height: 4
    fig_width: 4.5
  html_document:
    fig_height: 4
    fig_width: 4.5
---


```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 
require(datasets)
# Some customization.  You can alter or delete as desired (if you know what you are doing).
trellis.par.set(theme=theme.mosaic()) # change default color scheme for lattice
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
```

#### <span style="color:Blue">**Intellectual Property:**</span>  
These problems are the intellectual property of the instructors and may not be reproduced outside of this course.

#### <span style="color:Crimson">**Due Date:**</span>  
Tuesday, Sep 19, 2017 at 11:59 PM 

***  

##########################################################################
## <span style="color:DarkViolet">Problem 1:  Model Assessment  </span>
##########################################################################

<span style="color:DarkViolet">This problem practices application of proper model assessment techniques, with a multiple linear regression model.</span>

<span style="color:DarkViolet">Download the data set *Trees.csv* [from Lesson 2 on D2L] and read it into R.  Reference with description of the *original* measurements may be found at: </span> https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/trees.html

```{r echo=FALSE}
str(trees)
```

<span style="color:DarkViolet">The general goal for this dataset is to predict Volume based on Girth and Height.  We will be fitting a predictive model using multiple linear regression.  The model is given below:
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_3\cdot Girth\cdot Height+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  
Note that there are five predictors, some of which are transformations of the original two variables Girth and Height, for predicting the value of the response variable Volume.</span>

### <span style="color:DarkViolet">Question 1</span> **<span style="color:Crimson">(3 points)</span>**
<span style="color:DarkViolet">Why is *Volume* the most reasonable response variable?  *Include real-world reasons (eg. physical practicalities) in your discussion.*</span>



<span style="color:green">**Text Answer**: </span>

***


### <span style="color:DarkViolet">Questions 2-7</span> **<span style="color:Crimson">(6 points, 1 each)</span>**
<span style="color:DarkViolet">Use multiple linear regression to find coefficient estimates:
</span>



```{r}
mod <- lm(Volume ~ Girth + Height + I(Girth*Height) + I(Girth**2) + I((Girth**2)*Height), data = trees)
summary(mod)

```

<span style="color:green">**Numeric Answer**  </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
$\beta_0 =$ 48.914179
$\beta_1 =$ -8.228180
$\beta_2 =$ -0.616152
$\beta_3 =$ 0.103075
$\beta_4 =$ 0.311160 
$\beta_5 =$ -0.001764  

### <span style="color:DarkViolet">Question 8</span> **<span style="color:Crimson">(2 points)</span>**
<span style="color:DarkViolet">How many of these predictor variables are significant?</span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**: 
0,    
1,  
2,  
3,  
4, or   
5

Answer: ZERO

*** 


<span style="color:DarkViolet">We now assess how useful the fitted model is, via k-fold cross-validation.</span>

### <span style="color:DarkViolet">Question 9</span> **<span style="color:Crimson">(1 point)</span>**
<span style="color:DarkViolet">In order to perform 5-fold cross-validation, how many separate models must be fit?</span>


<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**: 

1,  
5,  
6,  
31, or   
32

ANSWER: 5



### <span style="color:DarkViolet">Question 10</span> **<span style="color:Crimson">(2 points)</span>**
<span style="color:DarkViolet">Starting with:</span>

$\texttt{groups = c(rep(1:5,6),1)}$

<span style="color:DarkViolet">Set R’s seed to 2 (for Homework 2) and define cvgroups (random groups for the cross-validation) using the sample() function.  
Enter your R code below.</span>

<span style="color:green">**Code Answer**: </span>
```{r, echo=TRUE}
# Question 10
set.seed(2)
groups = c(rep(1:5,6),1)
n <- nrow(trees)
cvgroups <- sample(groups, n)

# Questions 11-12
k <- 5
allpredictedCV = rep(0,n)
for (i in 1:k)  {
  groupi = (cvgroups == i)
  lmfitCV <- lm(Volume ~ Girth + Height + I(Girth*Height) + I(Girth**2) + I((Girth**2)*Height), 
                data = trees,
                subset = !groupi)
  allpredictedCV[groupi] <- predict.lm(lmfitCV,trees[groupi,])
}
allpredictedCV

```


### <span style="color:DarkViolet">Question 11</span> **<span style="color:Crimson">(2 points)</span>**
<span style="color:DarkViolet">Use the 5-fold CV method to assess the model fit. Provide the predicted y-value for the **first** observation: </span>


<span style="color:green">**Numeric Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:


### <span style="color:DarkViolet">Question 12</span> **<span style="color:Crimson">(2 points)</span>**
<span style="color:DarkViolet">Use the 5-fold CV method to assess the model fit. Provide the predicted y-value for the **second** observation: </span>


<span style="color:green">**Numeric Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:

***


### <span style="color:DarkViolet">Question 13</span> **<span style="color:Crimson">(4 points)</span>**

<span style="color:DarkViolet">Calculate and report the $CV_{(5)}$ based on the 5-fold cross-validation: </span>


```{r}
# Question 13
CV5 <- sum((allpredictedCV-trees$Volume)^2)/n
CV5
```

<span style="color:green">**Numeric Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:



### <span style="color:DarkViolet">Question 14</span> **<span style="color:Crimson">(1 point)</span>**
<span style="color:DarkViolet">The MSE computed with the book’s formula is 5.70.  Note that this method assesses the model with the same data used to fit the model.  How does this compare to the value of $CV_{(5)}$ from the previous question? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of 

A) 	The MSE value is **greater**.

B) 	The MSE value is **less**.

C) 	The two values are about the **same**.



### <span style="color:DarkViolet">Question 15</span> **<span style="color:Crimson">(2 points)</span>**
<span style="color:DarkViolet">Is the MSE of 5.70 accurate?  **Explain why or why not.** </span>


<span style="color:green">**Text Answer**: </span>


### <span style="color:DarkViolet">Question 16</span> **<span style="color:Crimson">(3 points)</span>**
<span style="color:DarkViolet">Enter your R code for computing the $CV_{(5)}$ measure below.</span>

<span style="color:green">**Code Answer**: </span>
```{r echo=TRUE}
CV5 <- sum((allpredictedCV-trees$Volume)^2)/n
```


***  



**Bootstrapping**

<span style="color:DarkViolet"> We will now use the bootstrap to estimate variability of the coefficients.</span>

### <span style="color:DarkViolet">Question 17</span> **<span style="color:Crimson">(4 points)</span>**:

<span style="color:DarkViolet"> Program a function, making use of lm() to fit the linear regression model, that outputs the six coefficient estimates.  Set R’s seed to 2, and then use $\texttt{boot()}$ to produce R = 1000 bootstrap estimates for each of $\beta_0$, $\beta_1$, $\beta_2$, $\beta_3$, $\beta_4$, and $\beta_5$.  
Enter your R code below.</span>

<span style="color:green">**Code Answer**: </span>
```{r echo=TRUE}
# Question 17 
library(boot)
beta.fn <- function(inputdata,index) {
  lmfitboot = lm(Volume ~ Girth + Height + I(Girth*Height) + I(Girth**2) + I((Girth**2)*Height),
                 data=inputdata[index,])
  return(lmfitboot$coef)
}

set.seed(2)
bootoutput <- boot(trees,beta.fn,R=1000)
bootoutput

# Bootstrap Statistics :
#         original        bias     std. error
# t1* 48.914178608 -13.961072182 130.03305599
# t2* -8.228180223   2.653216677  20.77020206
# t3* -0.616152497   0.173537338   1.80913201
# t4*  0.103075071  -0.033028970   0.28116241
# t5*  0.311160154  -0.128803517   0.81741639
# t6* -0.001764375   0.001611177   0.01070411
```


### <span style="color:DarkViolet">Questions 18-23</span> **<span style="color:Crimson">(6 points, 1 each)</span>**:

<span style="color:DarkViolet">Use your bootstrap estimates to estimate the standard error, $SE(\beta_i)$, for each of i = 0, 1, 2, 3, 4, 5.</span>

<span style="color:green">**Numeric Answer**  </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
$SE(\hat{\beta_0}) =$   
$SE(\hat{\beta_1}) =$   
$SE(\hat{\beta_2}) =$   
$SE(\hat{\beta_3}) =$   
$SE(\hat{\beta_4}) =$   
$SE(\hat{\beta_5}) =$   


### <span style="color:DarkViolet">Question 24</span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">The standard errors estimated from usual linear regression methods are shown in the R output below:</span>

$\texttt{Coefficients:				}$

$\texttt{Variable       Estimate  Std. Error  t value	 PR(>|t|)}$

$\texttt{(Intercept)	 48.914179	90.852925	 0.538	   0.595}$

$\texttt{Girth	       -8.228180	13.803580	-0.596	   0.556}$

$\texttt{Height		     -0.616152	 1.250446	-0.493	   0.626}$

$\texttt{GirthHeight	  0.103075	 0.180291	 0.572	   0.573}$

$\texttt{Girth2	        0.311160	 0.536379	 0.580	   0.567}$

$\texttt{Girth2Height	 -0.001764	 0.006621	-0.266	   0.792}$

<span style="color:DarkViolet">How do these values compare to the standard errors computed in the previous set of questions? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of 

A) 	The estimates from usual linear regression methods are **greater**.

	
B)  The estimates from usual linear regression methods are **less**.

	
C) 	The two sets of estimates are about the **same**.


ANSWER: (B) LESS
***

## Problem 2 - Model Selection

<span style="color:DarkViolet">This problem practices application of proper model selection techniques, with a multiple linear regression model.
We will continue working with the predictive model using multiple linear regression.  However, we will now consider selection between 6 possible models:</span>

<span style="color:DarkViolet">Model 1: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_3\cdot Girth\cdot Height+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  
</span>

<span style="color:DarkViolet">Model 2: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height$  
</span>

<span style="color:DarkViolet">Model 3: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_3\cdot Girth\cdot Height$  
</span>

<span style="color:DarkViolet">Model 4: 
$Volume = \beta_0+\beta_1\cdot Girth +\beta_2\cdot Height+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  </span>

<span style="color:DarkViolet">Model 5: 
$Volume = \beta_0+\beta_4 \cdot Girth^2+\beta_5\cdot Girth^2\cdot Height$  
</span>

<span style="color:DarkViolet">Model 6: 
$Volume = \beta_0+\beta_5\cdot Girth^2\cdot Height$  
</span>

### <span style="color:DarkViolet">Questions 25-30</span> **<span style="color:Crimson">(6 points, 1 each)</span>**:

```{r}

model1 <- (Volume ~ Girth + Height + I(Girth*Height) + I(Girth**2) + I((Girth**2)*Height))
model2 <- (Volume ~ Girth + Height)
model3 <- (Volume ~ Girth + Height + I(Girth*Height))
model4 <- (Volume ~ Girth + Height + I(Girth**2) + I((Girth**2)*Height))
model5 <- (Volume ~ I(Girth**2) + I((Girth**2)*Height))
model6 <- (Volume ~ I(Girth**2)*Height)
allModels <- list(model1, model2, model3, model4, model5, model6)


n <- nrow(trees)
k = 31 
set.seed(2)
cvgroups = sample(seq(nrow(trees)))  #orders randomly, with seed (2) to determine starting point

numMods <- length(allModels)
allmodelCV = rep(NA,numMods) #place-holder for results
	#fits for each model m, including predictors 1:m in order of forward step regression selection

for (m in 1:numMods) {
  mPredfit = lm(formula = allModels[[m]],data=trees)
  #prediction via cross-validation
  allpredictedCV = rep(0,n)
  for (i in 1:k)  {
    #groupi = (cvgroups == i)
    lmfitCV = lm(formula = allModels[[m]],data=trees[-1,])
    #allpredictedCV[groupi] = predict.lm(lmfitCV,trees[1,])
    pred <- predict.lm(lmfitCV,trees[1,])
    allpredictedCV[i] <- mean(sum(pred - trees[1,'Volume'])^2)
  }
  allmodelCV[m] = mean(allpredictedCV)
}
allmodelCV
which(allmodelCV == min(allmodelCV))
```


<span style="color:DarkViolet">Use LOOCV (note n = 31) method to calculate $CV_{(31)}$ for each of Models 1-6.  Report the $CV_{(31)}$ for each model.</span>

<span style="color:green">**Numeric Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
For Model 1, $CV_{(31)}$ =  
For Model 2, $CV_{(31)}$ =  
For Model 3, $CV_{(31)}$ =  
For Model 4, $CV_{(31)}$ =  
For Model 5, $CV_{(31)}$ =  
For Model 6, $CV_{(31)}$ =  
(use code space in next question)  

### <span style="color:DarkViolet">Question 31</span> **<span style="color:Crimson">(4 points)</span>**:

<span style="color:DarkViolet"> Enter your R code for computing the $CV_{(31)}$ measure for Model 6 below. </span>

<span style="color:green">**Possible Answer**: </span>
```{r echo=TRUE}
# Model 5 had the lowest average mean squared error.


```


### <span style="color:DarkViolet">Question 32</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">Which model would you select based on the values of $CV_{(31)}$ for LOOCV? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of  
Model 1,  
Model 2,  
Model 3,  
Model 4,  
Model 5, or  
Model 6  

***


### <span style="color:DarkViolet">Question 33</span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">Explain why you chose the model selected in the previous question. </span>


<span style="color:green">**Text Answer**: </span>

### <span style="color:DarkViolet">Questions 34-39</span> **<span style="color:Crimson">(6 points, 1 each)</span>**:

<span style="color:DarkViolet">Using the same split of the data into five sets as you performed in Problem 1, use 5-fold cross-validation method to calculate $CV_{(5)}$  for each of Models 1-6.  Report the $CV_{(5)}$  for each model.</span>

```{r}
n <- nrow(trees)
k <- 5
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))  #produces list of group labels
set.seed(2)
cvgroups <- sample(groups, n)

# Questions 11-12
for (m in 1:numMods) {
  #prediction via cross-validation
  allpredictedCV = rep(0,n)
  for (i in 1:k)  {
    groupi = (cvgroups == i)
	  lmfitCV = lm(formula = allModels[[m]],data=trees[!groupi,])
    pred = predict.lm(lmfitCV,trees[groupi,])
    allpredictedCV[groupi] <- pred
  }
  allmodelCV[m] = sum((allpredictedCV-trees$Volume)^2)/n
}
allmodelCV
which(allmodelCV == min(allmodelCV))
```

<span style="color:green">**Numeric Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
For Model 1, $CV_{(5)}$ =  19.74168
For Model 2, $CV_{(5)}$ =  19.97314
For Model 3, $CV_{(5)}$ =  10.01294
For Model 4, $CV_{(5)}$ =  13.32378
For Model 5, $CV_{(5)}$ =  10.03437
For Model 6, $CV_{(5)}$ =  12.13205
(use code space above)  



### <span style="color:DarkViolet">Question 40</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">Which model would you select based on the values of $CV_{(5)}$ for 5-fold CV? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of  
Model 1,  
Model 2,  
Model 3,  
Model 4,  
Model 5, or  
Model 6  


```{r}
# ANSWER: Model 3
# Simply based on the values, model 3 is best because it has the lowest value (average MSE).
```

### <span style="color:DarkViolet">Question 41</span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">Explain why you chose the model selected in the previous question. </span>

<span style="color:green">**Text Answer**: </span>


### <span style="color:DarkViolet">Question 42</span> **<span style="color:Crimson">(3 points)</span>**:

<span style="color:DarkViolet">Considering the form of the model that was selected by cross-validation, why does this model make sense from a practical standpoint? </span>

<span style="color:green">**Text Answer**: </span>
```{}
ANSWER: This is a simple model in terms of number of interaction variables. Also, it is loosely similar to the mathematical formula for the volume of a cylinder.
```
*** 


## Problem 3 - Model Assessment & Selection with KNN

<span style="color:DarkViolet"> This problem practices application of proper model assessment and selection techniques, with the kNN model. </span> 

<span style="color:DarkViolet"> **Important**:  Use the FNN library for fitting K-nearest neighbors, to obtain consistent answers.</span>

<span style="color:DarkViolet"> In this problem, you will once again use the K-nearest neighbors approach to analyze the gas mileage of cars.  You will use the **Auto** data set from the ISLR package, along with the two new variables, **weight.std** and **year.std** (standardized values of the weight and year), that you created in Homework 1: K-Nearest Neighbors.</span>




### <span style="color:DarkViolet">Question 43</span> **<span style="color:Crimson">(3 points)</span>**:

<span style="color:DarkViolet"> **Model assessment**   </span>
<span style="color:DarkViolet"> Starting with: </span>

$\texttt{groups = c(rep(1:10,39),1,2)}$

<span style="color:DarkViolet"> Set R’s seed to 2 and use sample() to divide the data into ten sets.  Then use 10-fold cross-validation method to calculate $CV_{(10)}$  for 1-nearest neighbor regression. Remember to re-standardize each training set inside the cross-validation. Report the value.   <k/span>

<span style="color:green">**Numeric Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
<span style="color:DarkViolet"> $CV_{(10)}$ = </span>  
(use code space in next question)  


### <span style="color:DarkViolet">Question 44</span> **<span style="color:Crimson">(4 points)</span>**:

<span style="color:DarkViolet">Enter your R code for computing the $CV_{(10)}$ measure below. </span>

<span style="color:green">**Code Answer**: </span>
```{r echo=TRUE}
require(ISLR)
require(FNN)
require(dplyr)

k <- 10
n <- nrow(Auto)
groups = c(rep(1:k,floor(n/k)),1:(n-floor(n/k)*k))
set.seed(2)
cvgroups <- sample(groups, n)

CVresults <- rep(0, n)
for (i in 1:k)  {
  groupi = (cvgroups == i)
  weight.std <- Auto$weight[!groupi] %>% scale()
  year.std   <- Auto$year[!groupi] %>% scale()
  train.x <- cbind(weight.std, year.std)
  train.y <- Auto$mpg[!groupi]
  
  valid.weight.std <- (Auto$weight[groupi] - mean(Auto$weight[!groupi])) / sd(Auto$weight[!groupi])
  valid.year.std <- (Auto$year[groupi] - mean(Auto$year[!groupi])) / sd(Auto$year[!groupi])
  valid.x <- cbind(valid.weight.std, valid.year.std)
  valid.y <- Auto$mpg[groupi]
  
  knnCV <- knn.reg(train.x, valid.x, train.y, k = 1)
  
  CVresults[groupi] <- knnCV$pred
}
CV10 <- sum((CVresults-Auto$mpg)^2)/n; CV10

#CV10 <- sum((results$valid.y - results$knnCV.pred)^2)/nrow(results)

```



### <span style="color:DarkViolet">Question 45</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">In general, how should the $CV_{(10)}$ value compare to the value of MSE (computed by reusing the same data used to fit the model)?</span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of 
$CV_{(10)} > MSE$,  
$CV_{(10)} < MSE$, or  
$CV_{(10)} \approx MSE$

***

### <span style="color:DarkViolet">Question 46</span> **<span style="color:Crimson">(3 points)</span>**:

<span style="color:DarkViolet">Consider models 1-30 as the k-nearest neighbors regression for values of k from 1 to 30. Using the same split of the data into ten sets as you performed in the Model assessment section, use 10-fold cross-validation method to calculate CV(10) for each of Models 1-30; remember to re-standardize each training set inside the cross-validation. Make a plot of the CV(10) as a function of k.
Upload your plot to Homework 2: CV(10) Plot.  </span>

<span style="color:green">**Plot Answer - submit to D2L discussion board *Homework 2: CV(10) Plot* **: </span>
```{r echo=FALSE}
require(ISLR)
require(FNN)
require(dplyr)
require(ggplot2)
kf <- 10
KNN = seq(30)
n <- nrow(Auto)
groups = c(rep(1:kf,floor(n/kf)),1:(n-floor(n/kf)*kf))  #produces list of group labels
all.results <- data.frame()
set.seed(2)
cvgroups <- sample(groups, n)
for(kn in KNN){
  CVresults = rep(NA,n)
  for (i in 1:kf)  {
    groupi = (cvgroups == i)
    weight.std <- Auto$weight[!groupi] %>% scale()
    year.std   <- Auto$year[!groupi] %>% scale()
    train.x <- cbind(weight.std, year.std)
    train.y <- Auto$mpg[!groupi]
    
    valid.weight.std <- (Auto$weight[groupi] - mean(Auto$weight[!groupi])) / sd(Auto$weight[!groupi])
    valid.year.std <- (Auto$year[groupi] - mean(Auto$year[!groupi])) / sd(Auto$year[!groupi])
    valid.x <- cbind(valid.weight.std, valid.year.std)

    knnCV <- knn.reg(train.x, valid.x, train.y, k = kn)
    
    CVresults[groupi] <- knnCV$pred
  }
  CV10 <- sum((CVresults - Auto$mpg)^2)/n
  all.results <- rbind(all.results, data.frame(k=kn, CV10=CV10))
}
colVec <- all.results$CV10 == min(all.results$CV10)
ggplot(all.results, aes(x=k,y=CV10)) +
  geom_point(aes(col=!colVec)) +
  scale_x_continuous(name='K-Neighbors') +
  guides(col=F) +
  annotate('text',x = which(all.results$CV10 == min(all.results$CV10)),y=9, label='K=20')

ggsave('/Users/ahendel1/Documents/Academics/data-mining/CV/cvPlot.png')
  

```



### <span style="color:DarkViolet">Question 47</span> **<span style="color:Crimson">(2 points)</span>**:

<span style="color:DarkViolet">Which k (number of nearest neighbors) would you select based on the values of $CV_{(10)}$ for 10-fold CV?

 </span>

<span style="color:green">**Numeric (Integer) Answer** </span>  
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  
```
K=4 provides the lowest CV10 value but is also relatively small, which decreases model complexity.

It turns out k=4 is a local minima, which means both values lower and higher had higher CV10 values.
```

### <span style="color:DarkViolet">Question 48 </span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">Explain why you chose the k value specified in the previous question. *Comment on both model predictive ability and model complexity.*</span>

<span style="color:green">**Text Answer**: </span>



### <span style="color:DarkViolet">Question 49</span> **<span style="color:Crimson">(1 point)</span>**:

<span style="color:DarkViolet">How does your selected model's k (number of nearest neighbors) compare to the one you chose in Homework 1? </span>

<span style="color:green">**Multiple Choice Answer** </span>
**<span style="color:red">(AUTOGRADED on D2L)</span>**:  one of 
	
k is **greater** than in Homework 1, or

	
k is **less** than in Homework 1.












